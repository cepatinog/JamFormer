{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“’ Add paths and import required modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root and utils to sys.path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))           # for model.py\n",
    "sys.path.append(os.path.abspath(\"../utils\"))     # for utils.py, logger.py\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ChordConditionedMelodyTransformer\n",
    "from utils.utils import pitch_to_midi\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“’ Define the model configuration\n",
    "Make sure this matches your hparams.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (should match training)\n",
    "model_config = {\n",
    "    \"num_pitch\": 50,\n",
    "    \"frame_per_bar\": 16,\n",
    "    \"num_bars\": 8,\n",
    "    \"chord_emb_size\": 128,\n",
    "    \"pitch_emb_size\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"key_dim\": 512,\n",
    "    \"value_dim\": 512,\n",
    "    \"num_layers\": 8,\n",
    "    \"num_heads\": 16,\n",
    "    \"input_dropout\": 0.2,\n",
    "    \"layer_dropout\": 0.2,\n",
    "    \"attention_dropout\": 0.2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“’ Load the trained model from checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25210/3072443676.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint_path = \"/home/cepatinog/smc-assignments/final_project/my_jazz_project/results/idx002/model/checkpoint_70.pth.tar\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "model = ChordConditionedMelodyTransformer(**model_config)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded and ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“’ Chord helper function\n",
    "\n",
    "Convert chord names to 12-dimensional binary vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chord dictionary (expand as needed)\n",
    "def roman_to_chord_vector(roman):\n",
    "    chord_templates = {\n",
    "        'C': [0, 4, 7],\n",
    "        'F': [5, 9, 0],\n",
    "        'G': [7, 11, 2],\n",
    "        'Am': [9, 0, 4],\n",
    "        'Dm': [2, 5, 9],\n",
    "        'Em': [4, 7, 11]\n",
    "    }\n",
    "    vec = np.zeros(12)\n",
    "    for p in chord_templates.get(roman, []):\n",
    "        vec[p % 12] = 1\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“’ Create a custom chord progression and convert to tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… chord_tensor shape: torch.Size([1, 128, 12])\n"
     ]
    }
   ],
   "source": [
    "# 8 bars Ã— 16 frames per bar = 128 time steps\n",
    "progression = ['C', 'Am', 'F', 'G', 'C', 'Am', 'F', 'G']\n",
    "frames_per_bar = model_config[\"frame_per_bar\"]\n",
    "max_len = model_config[\"num_bars\"] * frames_per_bar\n",
    "\n",
    "chord_matrix = np.zeros((max_len, 12))\n",
    "for i, chord in enumerate(progression):\n",
    "    vec = roman_to_chord_vector(chord)\n",
    "    chord_matrix[i * frames_per_bar:(i + 1) * frames_per_bar] = vec\n",
    "\n",
    "chord_tensor = torch.tensor(chord_matrix).unsqueeze(0).float()  # shape: [1, 128, 12]\n",
    "print(f\"âœ… chord_tensor shape: {chord_tensor.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“’ Generate melody from scratch (no prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Melody generated.\n"
     ]
    }
   ],
   "source": [
    "# Empty prime (start from scratch)\n",
    "prime_rhythm = torch.zeros((1, 0), dtype=torch.long)\n",
    "prime_pitch = torch.zeros((1, 0), dtype=torch.long)\n",
    "\n",
    "# Sampling\n",
    "with torch.no_grad():\n",
    "    result = model.sampling(prime_rhythm, prime_pitch, chord_tensor, topk=3)\n",
    "\n",
    "pitch = result[\"pitch\"][0].numpy()\n",
    "rhythm = result[\"rhythm\"][0].numpy()\n",
    "print(\"âœ… Melody generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MIDI saved as generated_custom.mid\n"
     ]
    }
   ],
   "source": [
    "output_path = \"generated_custom.mid\"\n",
    "pitch_to_midi(pitch, chord_matrix, frame_per_bar=frames_per_bar, save_path=output_path)\n",
    "print(f\"âœ… MIDI saved as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Piano roll saved as generated_custom_pianoroll.png\n"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "\n",
    "midi_data = pretty_midi.PrettyMIDI(output_path)\n",
    "melody_roll = midi_data.instruments[0].get_piano_roll(fs=16)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(melody_roll, origin='lower', aspect='auto', cmap='gray_r')\n",
    "plt.title(\"Generated Melody - Piano Roll\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MIDI Pitch\")\n",
    "plt.savefig(\"generated_custom_pianoroll.png\")\n",
    "print(\"âœ… Piano roll saved as generated_custom_pianoroll.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
